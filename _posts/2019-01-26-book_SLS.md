---
layout: post
title:  "book :: Statistical Learning with Sparsity"
date:   2018-12-28
categories: book
permalink: book_SLS
tags: book regularization

# author
author: Kipoong Kim
---

Statistical Learning with Sparsity (2016)
The Lasso and Generalizations
-Trevor Hastie
-Robert Tibshirani
-Martin Wainwright

1. Introduction
2. The Lasso for Linear Models
3. Generalized Linear Models
4. Generalizations of the Lasso penalty
5. Optimization methods
6. Statistical Inference
7. Matrix Decompositions, Approximations, and Completion
8. Sparse Multivariate Methods
9. Graphs and Model Selection
10. Signal Approximation and Compressed Sensing
11. Theoretical Results for the Lasso

<!-- more -->


1.Introduction
---------

<!-- <div style="text-align: right"> *Journal of the Royal Statistical Society: series B (2005)* </div> -->

"The world is awash with data."

"We are drowning in information and starving for knowledge."

"There is a crucial need to sort through this mass of information, and pare it down to bare essentials."

<br>

We hope that not all of 30,000 or so genes in human body are directly involved in the process that leads to the development of cancer.

This points to an underlying assumption of simplicity. One form of simplicity is *sparsity*.

In this book, we study methods that exploit sparsity to help recover the underlying signal in a set of data.

### Least-squares

- Typically all of least-squares estimates will be nonzero. This will make interpretations of the final model challenging if p is large.

- In fact, if p>>N, the least-squares estimates are not unique. There is an infinite set of solutions that make the objective function equal to zero, and these solutions almost surely overfit the data as well.

### Regularization
<!-- <script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script> -->

#### why do we use the $$\ell_1$$ norm?
- With $$q>1$$, sparse solution vectors yielded by the lasso does not occur for $$\ell_q$$ norms.
- For $$q<1$$, the solutions are sparse but the problem is not convex and this make the minimization very challenging computationally.
- The value $$q=1$$ is the smallest value that yields a convex problem.

#### The advantages of sparsity
- Interpretaion of the fitted model
- Convenience
- The "bet of sparsity" principle
*"Use a procedure that does well in sparse problems, since no procedure does well in dens problems."*

, i.e, if the true model is sparse, it turns out that we can estimate the parameters effectively using the lasso and sparse models.

<!-- ![Wavelet Coefficients](./img/book_SLS(1).png) -->

<figure>
  <img src="./img/book_SLS(1).png" alt="Wavelet Coefficients" width="600">
  <figcaption>Figure 1.2</figcaption>
</figure>

<hr>

2.The Lasso for Linear Models
-----------------

### Linear regression

#### Linear model

$$\eta(x_i) = \beta_0 + \sum_{j=1}^p x_{ij} \beta_j. $$

The model is parametrized by the vector of regression weights
 $$\beta=(\beta_1, \dots, \beta_p)\in \mathbb{R}^p $$.

$$\qquad$$ The usal "least-squares" estimator for the pair $$(\beta_0,  \beta)$$ is based on minimizing squared-error loss:

$$ \underset{\beta_0, \beta}{\text{minimize}}{\frac{1}{2N}\sum_{i=1}^N(y_i-\beta_0 - \sum_{j=1}^p x_{ij}\beta_j )^2 }. $$

##### Limitations of linear model
- Prediction accuracy with large variance.
- Interpretation when $$p$$ is large

#### The Lasso estimator

Given a collection of $$N$$ predictor-response pairs $${(x_i, y_i)}_{i=1}^N$$, the lasso finds the solution $$(\hat{\beta}_0, \hat{\beta})$$ to the Optimization problem

$$ \underset{\beta_0, \beta}{\text{minimize}} \left\{ \frac{1}{2N}\sum_{i=1}^N (y_i - \beta_0 - \sum_{j=1}^p x_{ij}\beta_j )^2 \right\} \\
\text{subject to} \sum_{j=1}^p |\beta_j|\le t.
$$

Furthermore, the above equation is often represented using matrix-vector notation. Let $$\boldsymbol{y}=(y_1, \dots, y_N)$$ denote the N-vector of responses, and $$\boldsymbol{X}$$ be an $$N\times p$$ matrix with $$x_i \in \mathbb{R}^p$$ in its $$i^th$$ row, then the optimization problem can be re-expressed as

$$
\underset{\beta_0, \beta}{\text{minimize}}
\left\{ \frac{1}{2N} \parallel \boldsymbol{y}-\beta_0 \boldsymbol{1} - \boldsymbol{X}\beta \parallel_2^2 \right\} \\
\text{subject to} \parallel \beta \parallel_1 \le t,
$$

where **$$1$$** is the vector of N ones, and $$\parallel \cdot \parallel_2$$ denotes the usual Euclidean norm on vectors.

##### Procedures
1. Standarization
    - Standardize the predictors so that each column is centered $$\frac{1}{N}\sum_{i=1}^N x_{ij}=0)$$ and scaled $$\frac{1}{N}\sum_{i=1}^Nx_{ij}^2 =1 ).$$ Without standarization, the lasso solutions would depend on the units used to measure the predictors.
    - We also assume that the response $$y_i$$ have been centered, meaning that $$\frac{1}{N}\sum_{i=1}^N y_i = 0$$. These centering conditions are convenient, since they mean that we can omit the intercept term $$\beta_0$$ in the lasso optimization.

It is often convenient to rewrite the lasso problem in the so-called Lagrangian form

$$
\underset{\beta \in \mathbb{R}^p}{\text{minimize}} \left\{ \frac{1}{2N} \parallel y-\boldsymbol{X}\beta \parallel_2^2 + \lambda \parallel \beta \parallel_1 \right\}
$$

for some $$\lambda\ge 0$$. By Lagrangian duality, there is a one-to-one correspondence between the constrained problem and the Lagrangian form: for each value of $$t$$ in the range where the constrain $$\parallel \beta \parallel_1 \le t $$ is active, there is a corresponding value of $$\lambda$$ that yields the same solution from the Lagrangian form with $$ t=\parallel \hat{\beta_\lambda} \parallel_1 $$

The theory of convex analysis tells us that necessary and sufficient conditions for a solution to the lasso problem take the form

$$
-\frac{1}{N}<x_j, y-\boldsymbol{X}\beta> + \lambda s_j = 0, j=1, \dots, p.
$$

Here each $$s_j$$ is an unknown quantity equal to $$\text{sign}(\beta_j)$$ if $$\beta_j \ne 0$$ and some value lying in $$[-1, 1]$$ otherwise--that is, it is a subgradient for the absolute value function. In other words, the solutions $$\hat{\beta}$$ to the lasso problem are the same as solutions $$(\hat{\beta}, \hat{s})$$ to the above equation. This system is a form of the so-called Karush-Kuhn-Tucker (KKT) conditions for problem.


<br>
<hr>
<br>

3. Generalized Linear Models
4. Generalizations of the Lasso penalty
5. Optimization methods
6. Statistical Inference
7. Matrix Decompositions, Approximations, and Completion
8. Sparse Multivariate Methods
9. Graphs and Model Selection
10. Signal Approximation and Compressed Sensing
11. Theoretical Results for the Lasso
