---
layout: post
title:  "book :: Advanced Statistical Computing"
date:   2019-03-13
categories: book
permalink: book_ASC
tags: book optimization

# author
author: Roger D. Peng
---

Advanced Statistical Computing (2018)
-Roger D. Peng

1. Introduction (Textbooks vs. Computers)
2. Solving Nonlinear Equations
  - Bisection Algorithm
  - Rates of Convergence
  - Functional Iteration
  - Newton's Method
3. General Optimization
  - Steepest Descent
  - The Newton Direction
  - Quasi-Newton
  - Conjugate Gradient
  - Coordinate Descent
4. The EM Algorithm
5. Integration
  - Laplace Approximation
6. Independent Monte Carlo
  - Random Number Generation
  - Non-Uniform Random Numbers
  - Rejection Sampling
  - Importance Sampling
7. Markov Chain Monte Carlo
  - Metropolis-Hastings
  - Gibbs Sampler

<!-- more -->


1.Introduction
---------

<!-- <div style="text-align: right"> *Journal of the Royal Statistical Society: series B (2005)* </div> -->

## The purpose of this book is to focus on one particular aspect: the development and implementation of statistical algorithm.

<figure>
  <img src="./img/book_ASC/Figure1.png" alt="The process of statistical modeling" width="600">
  <figcaption>Figure 1.1</figcaption>
</figure>




"The world is awash with data."

"We are drowning in information and starving for knowledge."

"There is a crucial need to sort through this mass of information, and pare it down to bare essentials."

<br>

We hope that not all of 30,000 or so genes in human body are directly involved in the process that leads to the development of cancer.

This points to an underlying assumption of simplicity. One form of simplicity is *sparsity*.

In this book, we study methods that exploit sparsity to help recover the underlying signal in a set of data.

### Least-squares

- Typically all of least-squares estimates will be nonzero. This will make interpretations of the final model challenging if p is large.

- In fact, if p>>N, the least-squares estimates are not unique. There is an infinite set of solutions that make the objective function equal to zero, and these solutions almost surely overfit the data as well.

### Regularization
<!-- <script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script> -->

#### why do we use the $$\ell_1$$ norm?
- With $$q>1$$, sparse solution vectors yielded by the lasso does not occur for $$\ell_q$$ norms.
- For $$q<1$$, the solutions are sparse but the problem is not convex and this make the minimization very challenging computationally.
- The value $$q=1$$ is the smallest value that yields a convex problem.

#### The advantages of sparsity
- Interpretaion of the fitted model
- Convenience
- The "bet of sparsity" principle
*"Use a procedure that does well in sparse problems, since no procedure does well in dens problems."*

, i.e, if the true model is sparse, it turns out that we can estimate the parameters effectively using the lasso and sparse models.

<!-- ![Wavelet Coefficients](./img/book_SLS(1).png) -->

<figure>
  <img src="./img/book_SLS(1).png" alt="Wavelet Coefficients" width="600">
  <figcaption>Figure 1.2</figcaption>
</figure>

<hr>

2.The Lasso for Linear Models
-----------------

### Linear regression

#### Linear model

$$\eta(x_i) = \beta_0 + \sum_{j=1}^p x_{ij} \beta_j. $$

The model is parametrized by the vector of regression weights
 $$\beta=(\beta_1, \dots, \beta_p)\in \mathbb{R}^p $$.

$$\qquad$$ The usal "least-squares" estimator for the pair $$(\beta_0,  \beta)$$ is based on minimizing squared-error loss:

$$ \underset{\beta_0, \beta}{\text{minimize}}{\frac{1}{2N}\sum_{i=1}^N(y_i-\beta_0 - \sum_{j=1}^p x_{ij}\beta_j )^2 }. $$

##### Limitations of linear model
- Prediction accuracy with large variance.
- Interpretation when $$p$$ is large

#### The Lasso estimator

Given a collection of $$N$$ predictor-response pairs $${(x_i, y_i)}_{i=1}^N$$, the lasso finds the solution $$(\hat{\beta}_0, \hat{\beta})$$ to the Optimization problem

$$ \underset{\beta_0, \beta}{\text{minimize}} \left\{ \frac{1}{2N}\sum_{i=1}^N (y_i - \beta_0 - \sum_{j=1}^p x_{ij}\beta_j )^2 \right\} \\
\text{subject to} \sum_{j=1}^p |\beta_j|\le t.
$$

Furthermore, the above equation is often represented using matrix-vector notation. Let $$\boldsymbol{y}=(y_1, \dots, y_N)$$ denote the N-vector of responses, and $$\boldsymbol{X}$$ be an $$N\times p$$ matrix with $$x_i \in \mathbb{R}^p$$ in its $$i^th$$ row, then the optimization problem can be re-expressed as

$$
\underset{\beta_0, \beta}{\text{minimize}}
\left\{ \frac{1}{2N} \parallel \boldsymbol{y}-\beta_0 \boldsymbol{1} - \boldsymbol{X}\beta \parallel_2^2 \right\} \\
\text{subject to} \parallel \beta \parallel_1 \le t,
$$

where **$$1$$** is the vector of N ones, and $$\parallel \cdot \parallel_2$$ denotes the usual Euclidean norm on vectors.

#### Standarization
    - Standardize the predictors so that each column is centered $$\frac{1}{N}\sum_{i=1}^N x_{ij}=0)$$ and scaled $$\frac{1}{N}\sum_{i=1}^Nx_{ij}^2 =1 ).$$ Without standarization, the lasso solutions would depend on the units used to measure the predictors.
    - We also assume that the response $$y_i$$ have been centered, meaning that $$\frac{1}{N}\sum_{i=1}^N y_i = 0$$. These centering conditions are convenient, since they mean that we can omit the intercept term $$\beta_0$$ in the lasso optimization.

It is often convenient to rewrite the lasso problem in the so-called Lagrangian form

$$
\underset{\beta \in \mathbb{R}^p}{\text{minimize}} \left\{ \frac{1}{2N} \parallel y-\boldsymbol{X}\beta \parallel_2^2 + \lambda \parallel \beta \parallel_1 \right\}
$$

for some $$\lambda\ge 0$$. By Lagrangian duality, there is a one-to-one correspondence between the constrained problem and the Lagrangian form: for each value of $$t$$ in the range where the constrain $$\parallel \beta \parallel_1 \le t $$ is active, there is a corresponding value of $$\lambda$$ that yields the same solution from the Lagrangian form with $$ t=\parallel \hat{\beta_\lambda} \parallel_1 $$

The theory of convex analysis tells us that necessary and sufficient conditions for a solution to the lasso problem take the form

$$
-\frac{1}{N}<x_j, y-\boldsymbol{X}\beta> + \lambda s_j = 0, j=1, \dots, p.
$$

Here each $$s_j$$ is an unknown quantity equal to $$\text{sign}(\beta_j)$$ if $$\beta_j \ne 0$$ and some value lying in $$[-1, 1]$$ otherwise--that is, it is a subgradient for the absolute value function. In other words, the solutions $$\hat{\beta}$$ to the lasso problem are the same as solutions $$(\hat{\beta}, \hat{s})$$ to the above equation. This system is a form of the so-called Karush-Kuhn-Tucker (KKT) conditions for problem.

#### Solution paths

![Solution paths for the lasso and ridge](./img/book_SLS(2).png)

![Solution paths for the lasso and ridge](./img/book_SLS(3).png)


#### The *relaxed lasso*

<div style="text-align: right"> (Meinshausen 2007) </div>

##### Two-stage process

1. Fitting the lasso and getting **which variables** have **nonzero coefficients**.

2. **Debiasing** the nonzero estimates **using least squares** for the variables selected by the lasso.

#### Cross-validation and Inference

- The bound t in the lasso criterion controls the complexity of the model; larger values of t free up more parameters and allow the model to adapt more closely to the training data. Conversely, smaller values of t restrict the parameters more, leading to sparser, more interpretable models that fit the data less closely.

- A value of t that is too small can prevent the lasso from capturing the main signal in the data, while too large a value can lead to overfitting.

- In order to estimate this best value for t, we can create artificial training and test sets by splitting up the given dataset at random, and estimating performance on the test data, using a procedure known as *cross-validation*.

#### Computation of the Lasso Solutions

##### Single predictor: Soft Thresholding
The standard approach to solve the lasso problem would be to take the gradient (first derivative) with respect to $$\beta$$, and set it zero.
But, for the absolute value function $$|\beta|$$,

$$
\hat{\beta}=
\begin{cases}
\frac{1}{N}<z,y>-\lambda & \text{if } \frac{1}{N}<z,y> > \lambda,\\
0 & \text{if } \frac{1}{N}|<z,y>| < \lambda,\\
\frac{1}{N}<z,y>+\lambda & \text{if } \frac{1}{N}<z,y> < -\lambda,
\end{cases}
$$, which can write succinctly as

$$
\hat{\beta}=S_\lambda(\frac{1}{N}<z, y>).
$$

Here, the *soft-thresholding* operator

$$
S_{\lambda} (x) = \text{sign}(x)\left( |x|-\lambda \right)_+
$$

##### Multiple predictor: Soft Thresholding

![Soft thresholding function](../img/book_SLS(4).png)

1. Cyclic coordinate descent algorithm

$$
\frac{1}{2N} \sum_{i=1}^N (y_i - \sum_{k \ne j}x_{ik}\beta_k - x_{ij}\beta_j )^2 + \lambda \sum_{k \ne j} |\beta_k| + \lambda |\beta_j|,
$$

We see that solution for each $$\beta_j$$ can be expressed succinctly in terms of the *partial residual* $$r_i^{(j)}=y_i-\sum_{k \ne j} x_{ik}\hat{\beta}_k$$, which removes from the outcome the current fit from all but the $$j^{\text{th}}$$ predictor. In terms of this partial residual, the $$j^{\text{th}}$$ coefficient is updated as

$$
\hat{\beta}_j = \boldsymbol(S)_\lambda \left( \frac{1}{N}<\boldsymbol(x_j), \boldsymbol(r)^{(j)}> \right),
$$

where $$r_i = y_i - \sum_{j=1}^p x_{ij}\hat{\beta}_j $$ are the full residuals.

The criterion for the lasso is convex function of $$\beta$$ and so has no local minima. Under relatively mild conditions, such coordinate-wise minimization schemes aplied to a convex function converge to a global optimum as follows.

![cyclic coordinate descent](./img/book_SLS(5).png)


- Furthermore, a method to compute the solutions over a grid of $$\lambda$$ value is refered as *pathwise coordinate descent*.


2. Homotopy methods

This will be described in Chapter 5.

##### Soft-thresholding and orthogonal bases

If the predictors are orthogonal, meaning that $$\frac{1}{N}<\boldsymbol{x_i}, \boldsymbol{x_j}> = 0$$ for each $$j\ne k$$. In this case the update simplifies dramatically, since $$\frac{1}{N}<\boldsymbol{x_j}, \boldsymbol{r}^{(j)}> = \frac{1}{N}<\boldsymbol{x_j}, \boldsymbol{y}>$$ so that $$\hat{\beta}_j$$ is simply the soft-thresholded version of the univariate least-squares estimate of **y** regressed against $$\boldsymbol{x_j}$$.

#### Degree of Freedom

If the $N$ samples predictions are denoted by $\hat{y}$, then We define

$$ \text{df}(\hat{y}) := \frac{1}{\sigma^2} \sum_{i=1}^N \text{Cov}(\hat{y_i}, y_i). $$

The covariance here is taken voer the randomness in the response variables $\{y_i\}_{i=1}^N$ with the predictors held fixed.

In linear regression, the degree of freedom of predictor is equal to


<br>
<hr>
<br>

3. Generalized Linear Models
4. Generalizations of the Lasso penalty
5. Optimization methods
6. Statistical Inference
7. Matrix Decompositions, Approximations, and Completion
8. Sparse Multivariate Methods
9. Graphs and Model Selection
10. Signal Approximation and Compressed Sensing
11. Theoretical Results for the Lasso
